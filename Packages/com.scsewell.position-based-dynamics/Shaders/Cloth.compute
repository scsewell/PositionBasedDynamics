#include "HLSLSupport.cginc"
#include "ClothCore.hlsl"

//#pragma use_dxc
#pragma enable_d3d11_debug_symbols

#pragma kernel CSMain

// TODO: decrease size? would need to use global memory to share results,
// or ensure groups are entirely separate chunks of verts/tris (how to handle overlap?)
#define THREAD_GROUP_SIZE       1024
#define SHARED_MEMORY_SIZE      ((32 * 1024) / 4)
#define MAX_SHARED_POSITIONS    (SHARED_MEMORY_SIZE / 2)
#define CURR_POSITIONS_OFFSET   0
#define PREV_POSITIONS_OFFSET   (MAX_SHARED_POSITIONS / 2)

groupshared uint gs_Data[SHARED_MEMORY_SIZE];

void StorePosition(uint index, CompressedPosition position)
{
    gs_Data[(2 * index) + 0] = position.packedData.x;
    gs_Data[(2 * index) + 1] = position.packedData.y;
}

CompressedPosition LoadPosition(uint index)
{
    CompressedPosition position;
    position.packedData.x = gs_Data[(2 * index) + 0];
    position.packedData.y = gs_Data[(2 * index) + 1];
    return position;
}

void StoreCurrPosition(uint index, CompressedPosition position)
{
    StorePosition(CURR_POSITIONS_OFFSET + index, position);
}

void StorePrevPosition(uint index, CompressedPosition position)
{
    StorePosition(PREV_POSITIONS_OFFSET + index, position);
}

CompressedPosition LoadCurrPosition(uint index)
{
    return LoadPosition(CURR_POSITIONS_OFFSET + index);
}

CompressedPosition LoadPrevPosition(uint index)
{
    return LoadPosition(PREV_POSITIONS_OFFSET + index);
}

void Integrate(inout float3 currentPosition, inout float3 previousPosition, uint inverseMass)
{
    float3 posDeltaFromVelocity = currentPosition - previousPosition;
    float3 posDeltaFromAccelration = _Gravity * _SubStepDeltaTime * _SubStepDeltaTime;
    float3 posDelta = posDeltaFromVelocity + posDeltaFromAccelration;
    
    previousPosition = currentPosition;
    currentPosition += inverseMass * posDelta;
}

void SolveDistanceConstraints(uint threadID, uint localID, uint groupID)
{
    // Process constraints in batches where no two constraints in the same batch
    // affect the same particles. This avoids the need to write atomically.

    // todo: profile unroll, see if registr increase is a real problem
    [unroll(MAX_CONSTRAINT_BATCHES)]
    for (uint batch = 0; batch < _ConstraintBatchCount; batch++)
    {
        uint3 batchData = _ConstraintBatchData[batch].xyz;
        uint batchOffset = batchData.x;
        uint batchSize = batchData.y;
        float compliance = asfloat(batchData.z);
        
        DistanceConstraint constraint = LoadDistanceConstraint(threadID, batchOffset);

        uint w0;
        uint w1;
        float3 p0 = DecompressPosition(LoadCurrPosition(constraint.indices.x), w0);
        float3 p1 = DecompressPosition(LoadCurrPosition(constraint.indices.y), w1);
        
        float3 disp = p0 - p1;
        float len = length(disp);
        float3 dir = len != 0 ? disp / len : 0;
        
        float c = len - constraint.restLength;
        float alpha = compliance / (_SubStepDeltaTime * _SubStepDeltaTime);
        float w = w0 + w1;
        float s = -c / (w + alpha);
        
        p0 += dir * (s * w0);
        p1 -= dir * (s * w1);
        
        if (threadID < batchSize)
        {
            StoreCurrPosition(constraint.indices.x, CompressPosition(p0, w0));
            StoreCurrPosition(constraint.indices.y, CompressPosition(p1, w1));
        }
        
        GroupMemoryBarrierWithGroupSync();
    }
}

void SumTriangleNormals(uint triangleIndex)
{
    // TODO: consider groupshared memory for normals?
    // TODO: compress normals
    uint3 tri = LoadTriangle(triangleIndex);

    float3 p0 = DecompressPosition(LoadCurrPosition(tri.x));
    float3 p1 = DecompressPosition(LoadCurrPosition(tri.y));
    float3 p2 = DecompressPosition(LoadCurrPosition(tri.z));

    float3 normal = normalize(cross(p1 - p0, p2 - p0));
    
    uint seed = tri.x + tri.y + tri.z;

    if (triangleIndex < _TriangleCount)
    {
        AtomicAdd(_Normals, tri.x, normal, seed);
        AtomicAdd(_Normals, tri.y, normal, seed);
        AtomicAdd(_Normals, tri.z, normal, seed);
    }
}

void SumTriangleNormals(uint threadID, uint localID, uint groupID)
{
    // TODO: graph coloring to avoid atomics?
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;
        
        _Normals[index] = asuint(float4(0, 0, 0, 0));
    }
    
    DeviceMemoryBarrierWithGroupSync(); 

    uint totalThreads = THREAD_GROUP_SIZE * _ThreadGroupCount;

    //uint trianglesPerThread = _TriangleCount / (THREAD_GROUP_SIZE * _ThreadGroupCount);
    uint trianglesPerThread = ((_TriangleCount - 1) / totalThreads) + 1;
    
    for (uint i = 0; i < trianglesPerThread; i++)
    {
        uint index = (trianglesPerThread * ((THREAD_GROUP_SIZE * groupID) + threadID)) + i;
        SumTriangleNormals(index);
    }
    
    DeviceMemoryBarrierWithGroupSync(); 
}

void StoreMeshVertex(uint index, float3 position, float3 normal)
{
    uint byteAddress = 3 * 4 * index;
    _MeshPositions.Store3(byteAddress, asuint(position));
    _MeshNormals.Store3(byteAddress, asuint(normal));
}

[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void CSMain(uint threadID : SV_DispatchThreadID, uint localID : SV_GroupThreadID, uint groupID : SV_GroupID)
{
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;

        StoreCurrPosition(index, _CurrentPositions[index]);
        StorePrevPosition(index, _PreviousPositions[index]);
    }
    
    for (uint i = 0; i < _SubStepCount; i++)
    {
        UNITY_UNROLL
        for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
        {
            uint index = (2 * threadID) + p;

            uint inverseMass;
            float3 currPos = DecompressPosition(LoadCurrPosition(index), inverseMass);
            float3 prevPos = DecompressPosition(LoadPrevPosition(index));
        
            Integrate(currPos, prevPos, inverseMass);
            
            StoreCurrPosition(index, CompressPosition(currPos, inverseMass));
            StorePrevPosition(index, CompressPosition(prevPos));
        }

        // TODO: may need to write more positions per thread if there are more positions allowed
        // I suppose we can write the positions to global memory, then each group loads all into LDS
        GroupMemoryBarrierWithGroupSync();

        SolveDistanceConstraints(threadID, localID, groupID);
    }

    SumTriangleNormals(threadID, localID, groupID);
    
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;

        CompressedPosition currPosComp = LoadCurrPosition(index);
        CompressedPosition prevPosComp = LoadPrevPosition(index);

        float3 position = DecompressPosition(currPosComp);
        float3 normalSum = asfloat(_Normals[index]);
        float3 normal = normalize(normalSum);
        
        if (threadID < _ParticleCount)
        {
            _CurrentPositions[index] = currPosComp;
            _PreviousPositions[index] = prevPosComp;

            StoreMeshVertex(index, position, normal);
        }
    }
}
