#include "HLSLSupport.cginc"
#include "ClothCore.hlsl"

//#pragma use_dxc
//#pragma enable_d3d11_debug_symbols

#pragma kernel CSMain

// TODO: decrease size? would need to use global memory to share results,
// or ensure groups are entirely separate chunks of verts/tris (how to handle overlap?)
#define THREAD_GROUP_SIZE       512
#define SHARED_MEMORY_SIZE      ((32 * 1024) / 4)
#define SHARED_PARTICLES_COUNT  (THREAD_GROUP_SIZE * PARTICLES_PER_THREAD)

groupshared float gs_InverseMasses[SHARED_PARTICLES_COUNT];
groupshared float3 gs_CurrPositions[SHARED_PARTICLES_COUNT];
groupshared float3 gs_PrevPositions[SHARED_PARTICLES_COUNT];

void StoreInverseMass(uint index, float inverseMass)
{
    if (index < SHARED_PARTICLES_COUNT)
    {
        gs_InverseMasses[index] = inverseMass;
    }
}

void StoreCurrPosition(uint index, float3 position)
{
    if (index < SHARED_PARTICLES_COUNT)
    {
        gs_CurrPositions[index] = position;
    }
}

void StorePrevPosition(uint index, float3 position)
{
    if (index < SHARED_PARTICLES_COUNT)
    {
        gs_PrevPositions[index] = position;
    }
}

float LoadInverseMass(uint index)
{
    return gs_InverseMasses[index];
}

float3 LoadCurrPosition(uint index)
{
    return gs_CurrPositions[index];
}

float3 LoadPrevPosition(uint index)
{
    return gs_PrevPositions[index];
}

void LoadParicle(uint index, out float3 currPos, out float3 prevPos, out float inverseMass)
{
	inverseMass = _InverseMasses[index];
    currPos = _CurrentPositions[index].xyz;
    prevPos = _PreviousPositions[index].xyz;
}

void Integrate(inout float3 currPos, inout float3 prevPos, float inverseMass)
{
    float3 posDeltaFromVelocity = currPos - prevPos;
    float3 posDeltaFromAccelration = _Gravity * _SubStepDeltaTime * _SubStepDeltaTime;
    float3 posDelta = posDeltaFromVelocity + posDeltaFromAccelration;
    
    prevPos = currPos;
    currPos += inverseMass * posDelta;
}

void SolveDistanceConstraints(uint threadID, uint localID, uint groupID)
{
    // Process constraints in batches where no two constraints in the same batch
    // affect the same particles. This avoids the need to write atomically.

    // todo: profile unroll, see if registr increase is a real problem
    [unroll(MAX_CONSTRAINT_BATCHES)]
    for (uint batch = 0; batch < _ConstraintBatchCount; batch++)
    {
        uint3 batchData = _ConstraintBatchData[batch].xyz;
        uint batchOffset = batchData.x;
        uint batchSize = batchData.y;
        float compliance = asfloat(batchData.z);
        
        DistanceConstraint constraint = LoadDistanceConstraint(threadID, batchOffset);

        float w0 = LoadInverseMass(constraint.indices.x);
        float w1 = LoadInverseMass(constraint.indices.y);
        float3 p0 = LoadCurrPosition(constraint.indices.x);
        float3 p1 = LoadCurrPosition(constraint.indices.y);
        
        float3 disp = p0 - p1;
        float len = length(disp);
        float3 dir = len != 0 ? disp / len : 0;
        
        float c = len - constraint.restLength;
        float alpha = compliance / (_SubStepDeltaTime * _SubStepDeltaTime);
        float w = w0 + w1;
        float s = -c / (w + alpha);
        
        p0 += dir * (s * w0);
        p1 -= dir * (s * w1);
        
        if (threadID < batchSize)
        {
            StoreCurrPosition(constraint.indices.x, p0);
            StoreCurrPosition(constraint.indices.y, p1);
        }
        
        GroupMemoryBarrierWithGroupSync();
    }
}

void StoreBoundaryParticle(GroupData group, uint index)
{
	if (group.particlesInIndex <= index && index < group.particlesUniqueIndex)
	{
		uint sharedIndex = sharedInIndex + (index - group.particlesInIndex);

		_SharedCurrentPositions[(2 * sharedIndex) + 0].xyz = LoadCurrPos(index);
		_SharedPreviousPositions[(2 * sharedIndex) + 0].xyz = LoadPrevPos(index);
	}
	else if (group.particlesOutIndex <= index && index < group.particlesEndIndex)
	{
		uint sharedIndex = sharedOutIndex + (index - group.particlesOutIndex);

		_SharedCurrentPositions[(2 * sharedIndex) + 1].xyz = LoadCurrPos(index);
		_SharedPreviousPositions[(2 * sharedIndex) + 1].xyz = LoadPrevPos(index);
	}

    DeviceMemoryBarrierWithGroupSync();
}

void SolveBoundaryParticle(GroupData group, uint index)
{
	if (group.particlesInIndex <= index && index < group.particlesUniqueIndex)
	{
		uint sharedIndex = sharedInIndex + (index - group.particlesInIndex);

		_SharedCurrentPositions[(2 * sharedIndex) + 0].xyz = LoadCurrPos(index);
		_SharedPreviousPositions[(2 * sharedIndex) + 0].xyz = LoadPrevPos(index);
	}
	else if (group.particlesOutIndex <= index && index < group.particlesEndIndex)
	{
		uint sharedIndex = sharedOutIndex + (index - group.particlesOutIndex);

		_SharedCurrentPositions[(2 * sharedIndex) + 1].xyz = LoadCurrPos(index);
		_SharedPreviousPositions[(2 * sharedIndex) + 1].xyz = LoadPrevPos(index);
	}

    DeviceMemoryBarrierWithGroupSync();
}


void SumTriangleNormals(uint triangleIndex)
{
    // TODO: consider groupshared memory for normals?
    // TODO: compress normals
    uint3 tri = LoadTriangle(triangleIndex);

    float3 p0 = LoadCurrPosition(tri.x);
    float3 p1 = LoadCurrPosition(tri.y);
    float3 p2 = LoadCurrPosition(tri.z);

    float3 normal = normalize(cross(p1 - p0, p2 - p0));
    
    uint seed = tri.x + tri.y + tri.z;

    if (triangleIndex < _TriangleCount)
    {
        AtomicAdd(_Normals, tri.x, normal, seed);
        AtomicAdd(_Normals, tri.y, normal, seed);
        AtomicAdd(_Normals, tri.z, normal, seed);
    }
}

void SumTriangleNormals(uint threadID, uint localID, uint groupID)
{
    // TODO: graph coloring to avoid atomics?
	// Alternatively use vertex fans 
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;
        
        _Normals[index] = asuint(float4(0, 0, 0, 0));
    }
    
    DeviceMemoryBarrierWithGroupSync(); 

    uint totalThreads = THREAD_GROUP_SIZE * _ThreadGroupCount;

    //uint trianglesPerThread = _TriangleCount / (THREAD_GROUP_SIZE * _ThreadGroupCount);
    uint trianglesPerThread = ((_TriangleCount - 1) / totalThreads) + 1;
    
    for (uint i = 0; i < trianglesPerThread; i++)
    {
        uint index = (trianglesPerThread * ((THREAD_GROUP_SIZE * groupID) + threadID)) + i;
        SumTriangleNormals(index);
    }
    
	// todo make this not require device barrier 
    DeviceMemoryBarrierWithGroupSync();
}

void StoreParicle(GroupData group, uint index, float3 currPos, float3 prevPos)
{
    float3 normalSum = asfloat(_Normals[index]);
    float3 normal = normalize(normalSum);
    uint byteAddress = 3 * 4 * index;
    
    if (index < group.particlesEndIndex)
    {
        _CurrentPositions[index].xyz = currPos;
        _PreviousPositions[index].xyz = prevPos;
    	_MeshPositions.Store3(byteAddress, asuint(currPos));
    	_MeshNormals.Store3(byteAddress, asuint(normal));
    }
}

[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void CSMain(uint threadID : SV_DispatchThreadID, uint localID : SV_GroupThreadID, uint groupID : SV_GroupID)
{
	GroupData group = _Groups[groupID];

	uint i0 = (2 * threadID) + 0;
	uint i1 = (2 * threadID) + 1;
	
    float3 currPos0, currPos1;
    float3 prevPos0, prevPos1;
	float w0, w1;

    LoadParicle(i0, currPos0, prevPos0, w0);
    LoadParicle(i1, currPos1, prevPos1, w1);

	StoreInverseMass(i0, w0);
	StoreInverseMass(i1, w1);

    for (uint i = 0; i < _SubStepCount; i++)
    {
        Integrate(currPos0, prevPos0, w0);
        Integrate(currPos1, prevPos1, w1);
        
        StoreCurrPosition(i0, currPos0);
        StoreCurrPosition(i1, currPos1);

        GroupMemoryBarrierWithGroupSync();

        SolveDistanceConstraints(threadID, localID, groupID);

        currPos0 = LoadCurrPosition(i0);
        currPos1 = LoadCurrPosition(i1);

		if ()
    	{
        }
    }

    SumTriangleNormals(threadID, localID, groupID);
    
	StoreParicle(group, i0, currPos0, prevPos0);
	StoreParicle(group, i1, currPos1, prevPos1);
}
