#include "HLSLSupport.cginc"
#include "ClothCore.hlsl"

//#pragma use_dxc
//#pragma enable_d3d11_debug_symbols

#pragma kernel CSMain

// TODO: decrease size? would need to use global memory to share results,
// or ensure groups are entirely separate chunks of verts/tris (how to handle overlap?)
#define THREAD_GROUP_SIZE   1024
#define SHARED_MEMORY_SIZE  32 * 1024

//groupshared uint gs_Data[SHARED_MEMORY_SIZE];
groupshared float3 gs_Data[SHARED_MEMORY_SIZE / 12];

struct Particle
{
    float inverseMass;
    float3 currentPosition;
    float3 previousPosition;
};

/*
struct CompressedPosition
{
    uint2 packedData;
};

float3 DecompressPosition(uint compressedPosition, out uint bit)
{
    
}

uint CompressPosition(float3 position, uint bit)
{
    
}

struct CompressedParticle
{
    uint currentPosition;
    uint previousPosition;
};

Particle DecompressParticle(CompressedParticle compressedParticle)
{
    
}
*/

Particle LoadParticleGlobal(uint index)
{
    bool isValid = index < _ParticleCount;
    
    Particle particle;
    particle.inverseMass = isValid ? _InverseMasses[index] : 0;
    particle.currentPosition = isValid ? _CurrentPositions[index].xyz : 0;
    particle.previousPosition = isValid ? _PreviousPositions[index].xyz : 0;
    return particle;
}

void StoreParticleGlobal(uint index, Particle particle)
{
    if (index < _ParticleCount)
    {
        _CurrentPositions[index].xyz = particle.currentPosition;
        _PreviousPositions[index].xyz = particle.previousPosition;
    }
}

Particle LoadParticleLocal(uint index)
{
    bool isValid = index < _ParticleCount;

    float3 data0 = isValid ? gs_Data[index] : 0;
    float3 data1 = isValid ? gs_Data[(SHARED_MEMORY_SIZE / 24) + index] : 0;

    Particle particle;
    particle.inverseMass = asuint(data0.x) & 0x1;
    particle.currentPosition = data0;
    particle.previousPosition = data1;
    return particle;
}

void StoreParticleLocal(uint index, Particle particle)
{
    float3 data0 = particle.currentPosition;
    float3 data1 = particle.previousPosition;
    data0.x = asfloat((asuint(data0.x)& ~0x1) | uint(particle.inverseMass));
    
    if (index < _ParticleCount)
    {
        gs_Data[index] = data0;
        gs_Data[(SHARED_MEMORY_SIZE / 24) + index] = data1;
    }
}

void Integrate(inout Particle particle)
{
    float3 posDeltaFromVelocity = particle.currentPosition - particle.previousPosition;
    float3 posDeltaFromAccelration = _Gravity * _SubStepDeltaTime * _SubStepDeltaTime;
    float3 posDelta = posDeltaFromVelocity + posDeltaFromAccelration;
    
    particle.previousPosition = particle.currentPosition;
    
    if (particle.inverseMass > 0.5)
    {
        particle.currentPosition += posDelta;
    }
}

void SolveDistanceConstraints(uint threadID, uint localID, uint groupID)
{
    // Process constraints in batches where no two constraints in the same batch
    // affect the same particles. This avoids the need to write atomically.
    uint batchOffset = 0;

    //[unroll(MAX_CONSTRAINT_BATCHES)]
    for (uint batch = 0; batch < _ConstraintBatchCount; batch++)
    {
        uint batchSize = _ConstraintBatchSize[batch].x;
        float compliance = _ConstraintBatchCompliance[batch].x;
        
        DistanceConstraint constraint = LoadDistanceConstraint(threadID, batchOffset, batchSize);
        Particle p0 = LoadParticleLocal(constraint.indices.x);
        Particle p1 = LoadParticleLocal(constraint.indices.y);

        float3 disp = p0.currentPosition - p1.currentPosition;
        float len = length(disp);
        float3 dir = len != 0 ? disp / len : 0;
        
        float c = len - constraint.restLength;
        float alpha = compliance / (_SubStepDeltaTime * _SubStepDeltaTime);
        float w = p0.inverseMass + p1.inverseMass;
        float s = -c / (w + alpha);
        
        p0.currentPosition += dir * (s * p0.inverseMass);
        p1.currentPosition += dir * (-s * p1.inverseMass);
        
        if (threadID < batchSize)
        {
            StoreParticleLocal(constraint.indices.x, p0);
            StoreParticleLocal(constraint.indices.y, p1);
        }
        
        GroupMemoryBarrierWithGroupSync();
        
        batchOffset += batchSize;
    }
}

void SumTriangleNormals(uint triangleIndex)
{
    // TODO: use groupshared memory for positions
    // TODO: consider groupshared memory for normals?
    // TODO: compress normals
    uint3 tri = LoadTriangle(triangleIndex);

    float3 p0 = _CurrentPositions[tri.x];
    float3 p1 = _CurrentPositions[tri.y];
    float3 p2 = _CurrentPositions[tri.z];

    float3 normal = normalize(cross(p1 - p0, p2 - p0));
    
    uint seed = tri.x + tri.y + tri.z;

    if (triangleIndex < _TriangleCount)
    {
        AtomicAdd(_Normals, tri.x, normal, seed);
        AtomicAdd(_Normals, tri.y, normal, seed);
        AtomicAdd(_Normals, tri.z, normal, seed);
    }
}

void SumTriangleNormals(uint threadID, uint localID, uint groupID)
{
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;
        
        _Normals[index] = asuint(float4(0, 0, 0, 0));
    }
    
    DeviceMemoryBarrierWithGroupSync(); 

    uint totalThreads = THREAD_GROUP_SIZE * _ThreadGroupCount;

    
    //uint trianglesPerThread = _TriangleCount / (THREAD_GROUP_SIZE * _ThreadGroupCount);
    uint trianglesPerThread = ((_TriangleCount - 1) / totalThreads) + 1;
    
    for (uint i = 0; i < trianglesPerThread; i++)
    {
        uint index = (trianglesPerThread * ((THREAD_GROUP_SIZE * groupID) + threadID)) + i;
        SumTriangleNormals(index);
    }
    
    DeviceMemoryBarrierWithGroupSync(); 
}

void UpdateMeshParticle(uint index, Particle particle)
{
    uint byteAddress = 3 * 4 * index;

    // todo: use groupshared?
    float3 position = particle.currentPosition;
    float3 normalSum = index < _ParticleCount ? asfloat(_Normals[index]) : 0;
    float3 normal = normalize(normalSum);
    
    if (index < _ParticleCount)
    {
        _MeshPositions.Store3(byteAddress, asuint(position));
        _MeshNormals.Store3(byteAddress, asuint(normal));
    }
}

[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void CSMain(uint threadID : SV_DispatchThreadID, uint localID : SV_GroupThreadID, uint groupID : SV_GroupID)
{
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;
        
        StoreParticleLocal(index, LoadParticleGlobal(index));
    }

    for (uint i = 0; i < _SubStepCount; i++)
    {
        UNITY_UNROLL
        for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
        {
            uint index = (2 * threadID) + p;

            Particle particle = LoadParticleLocal(index);
            Integrate(particle);
            StoreParticleLocal(index, particle);
        }

        // TODO: may need to write more positions per thread if there are more positions allowed
        // I suppose we can write the positions to global memory, then each group loads all into LDS
        GroupMemoryBarrierWithGroupSync();

        SolveDistanceConstraints(threadID, localID, groupID);
    }
    
    SumTriangleNormals(threadID, localID, groupID);
    
    UNITY_UNROLL
    for (uint p = 0; p < PARTICLES_PER_THREAD; p++)
    {
        uint index = (2 * threadID) + p;
        
        Particle particle = LoadParticleLocal(index);
        StoreParticleGlobal(index, particle);
        UpdateMeshParticle(index, particle);
    }
}
